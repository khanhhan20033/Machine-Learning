{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-17T10:38:32.027085400Z",
     "start_time": "2024-06-17T10:38:31.955088900Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import rc\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "rc('text', usetex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-06-17T10:39:13.995565300Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load MNIST data\n",
    "(Xtrain, ytrain), (Xtest, ytest) = mnist.load_data()\n",
    "\n",
    "# Preprocess data\n",
    "Xtrain = Xtrain.reshape(-1, 28*28) / 255.0\n",
    "Xtest = Xtest.reshape(-1, 28*28) / 255.0\n",
    "\n",
    "# Train logistic regression model\n",
    "logreg = linear_model.LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial', max_iter=1000)\n",
    "logreg.fit(Xtrain, ytrain)\n",
    "\n",
    "# Test model\n",
    "y_pred = logreg.predict(Xtest)\n",
    "accuracy = accuracy_score(ytest, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f} %\")\n",
    "\n",
    "# Plot a few test images with their predictions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "axes = axes.ravel()\n",
    "for i in np.arange(0, 10):\n",
    "    axes[i].imshow(Xtest[i].reshape((28, 28)), cmap='gray')\n",
    "    axes[i].set_title(f'Predicted: {y_pred[i]}')\n",
    "    axes[i].axis('off')\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-17T10:38:21.998236300Z",
     "start_time": "2024-06-17T10:38:21.998236300Z"
    }
   },
   "outputs": [],
   "source": [
    "Xtrain = np.concatenate((np.ones((Xtrain.shape[0], 1))/255.0, Xtrain), axis = 1)\n",
    "Xtest = np.concatenate((np.ones((Xtest.shape[0], 1))/255.0, Xtest), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-16T04:52:10.258915300Z",
     "start_time": "2024-06-16T04:52:10.149939800Z"
    }
   },
   "outputs": [],
   "source": [
    "print(Xtrain_all.shape)\n",
    "logreg = linear_model.LogisticRegression(C=1e5, solver = 'lbfgs', multi_class = 'multinomial')\n",
    "logreg.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-06-16T04:52:10.160046200Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(Xtest)\n",
    "print \"Accuracy: %.2f %%\" %(100*accuracy_score(ytest, y_pred.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-06-16T04:52:10.170118700Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np  \n",
    "\n",
    "Z = np.random.rand(3, 4) \n",
    "\n",
    "e_Z = np.exp(Z - np.max(Z, axis = 1, keepdims = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-06-16T04:52:10.180239900Z"
    }
   },
   "outputs": [],
   "source": [
    "A = e_Z / e_Z.sum(axis = 1, keepdims = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-16T04:52:10.190351600Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np  \n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Compute softmax values for each sets of scores in V.\n",
    "    each column of V is a set of scores.    \n",
    "    Z: a numpy array of shape (N, C)\n",
    "    return a numpy array of shape (N, C)\n",
    "    \"\"\"\n",
    "    e_Z = np.exp(Z)\n",
    "    A = e_Z / e_Z.sum(axis = 1, keepdims = True)\n",
    "    return A\n",
    "\n",
    "def softmax_stable(Z):\n",
    "    \"\"\"\n",
    "    Compute softmax values for each sets of scores in Z.\n",
    "    each row of Z is a set of scores.    \n",
    "    \"\"\"\n",
    "    # Z = Z.reshape(Z.shape[0], -1)\n",
    "    e_Z = np.exp(Z - np.max(Z, axis = 1, keepdims = True))\n",
    "    A = e_Z / e_Z.sum(axis = 1, keepdims = True)\n",
    "    return A\n",
    "\n",
    "# cost or loss function  \n",
    "def softmax_loss(X, y, W):\n",
    "    \"\"\"\n",
    "    W: 2d numpy array of shape (d, C), \n",
    "        each column correspoding to one output node\n",
    "    X: 2d numpy array of shape (N, d), each row is one data point\n",
    "    y: 1d numpy array -- label of each row of X \n",
    "    \"\"\"\n",
    "    A = softmax_stable(X.dot(W))\n",
    "    id0 = range(X.shape[0])\n",
    "    return -np.mean(np.log(A[id0, y]))\n",
    "\n",
    "# W_init = np.random.randn(d, C)\n",
    "\n",
    "def softmax_grad(X, y, W):\n",
    "    \"\"\"\n",
    "    W: 2d numpy array of shape (d, C), \n",
    "        each column correspoding to one output node\n",
    "    X: 2d numpy array of shape (N, d), each row is one data point\n",
    "    y: 1d numpy array -- label of each row of X \n",
    "    \"\"\"\n",
    "    A = softmax_stable(X.dot(W)) # shape of (N, C)\n",
    "    id0 = range(X.shape[0])\n",
    "    A[id0, y] -= 1  # A - Y, shape of (N, C)\n",
    "    return X.T.dot(A)/X.shape[0]\n",
    "    \n",
    "def numerical_grad(X, Y, W, loss):\n",
    "    eps = 1e-6\n",
    "    g = np.zeros_like(W)\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            W_p = W.copy()\n",
    "            W_n = W.copy()\n",
    "            W_p[i, j] += eps \n",
    "            W_n[i, j] -= eps\n",
    "            g[i,j] = (loss(X, Y, W_p) - loss(X, Y, W_n))/(2*eps)\n",
    "    return g \n",
    "\n",
    "def softmax_fit(X, y, W, lr = 0.01, nepoches = 100, tol = 1e-5, batch_size = 10):\n",
    "    W_old = W.copy()\n",
    "    ep = 0 \n",
    "    loss_hist = [softmax_loss(X, y, W)] # store history of loss \n",
    "    N = X.shape[0]\n",
    "    nbatches = int(np.ceil(float(N)/batch_size))\n",
    "    while ep < nepoches: \n",
    "        ep += 1 \n",
    "        mix_ids = np.random.permutation(N) # mix data \n",
    "        for i in range(nbatches):\n",
    "            # get the i-th batch\n",
    "            batch_ids = mix_ids[batch_size*i:min(batch_size*(i+1), N)] \n",
    "            X_batch, y_batch = X[batch_ids], y[batch_ids]\n",
    "            W -= lr*softmax_grad(X_batch, y_batch, W) # update gradient descent \n",
    "        loss_hist.append(softmax_loss(X, y, W))\n",
    "        if np.linalg.norm(W - W_old)/W.size < tol:\n",
    "            break \n",
    "        W_old = W.copy()\n",
    "    return W, loss_hist \n",
    "\n",
    "\n",
    "def pred(W, X):\n",
    "    \"\"\"\n",
    "    predict output of each columns of X\n",
    "    Class of each x_i is determined by location of max probability\n",
    "    Note that class are indexed by [0, 1, 2, ...., C-1]\n",
    "    \"\"\"\n",
    "    A = softmax_stable(X.dot(W))\n",
    "    return np.argmax(A, axis = 1)\n",
    "\n",
    "\n",
    "d = 100\n",
    "C = 3 \n",
    "N = 3000\n",
    "X = np.random.randn(N, d)\n",
    "y = np.random.randint(0, C, N) \n",
    "W = np.random.randn(d, C) \n",
    "# print(y.shape)\n",
    "# print(loss(X, y, W))\n",
    "# g1 = grad(X, y, W)\n",
    "# g2 = numerical_grad(X, y, W, loss)\n",
    "\n",
    "# # print(g1)\n",
    "# # print(g2)\n",
    "# print(np.linalg.norm(g1 - g2)/g1.size)\n",
    "\n",
    "W, loss_hist = softmax_fit(X, y, W, batch_size = 100, lr= 0.05)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(loss_hist[-1])\n",
    "plt.plot(loss_hist)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-16T04:52:10.200439Z"
    }
   },
   "outputs": [],
   "source": [
    "C = 5    # number of classes\n",
    "N = 500  # number of points per class \n",
    "means = [[2, 2], [8, 3], [3, 6], [14, 2], [12, 8]]\n",
    "cov = [[1, 0], [0, 1]]\n",
    "\n",
    "X0 = np.random.multivariate_normal(means[0], cov, N)\n",
    "X1 = np.random.multivariate_normal(means[1], cov, N)\n",
    "X2 = np.random.multivariate_normal(means[2], cov, N)\n",
    "X3 = np.random.multivariate_normal(means[3], cov, N)\n",
    "X4 = np.random.multivariate_normal(means[4], cov, N)\n",
    "\n",
    "X = np.concatenate((X0, X1, X2, X3, X4), axis = 0) # each row is a datapoint\n",
    "Xbar = np.concatenate((X, np.ones((X.shape[0], 1))), axis = 1) # bias trick \n",
    "\n",
    "y = np.asarray([0]*N + [1]*N + [2]*N+ [3]*N + [4]*N)\n",
    "\n",
    "W_init = np.random.randn(Xbar.shape[1], C)\n",
    "W, loss_hist = softmax_fit(Xbar, y, W_init, batch_size = 10, nepoches = 100, lr = 0.05)\n",
    "\n",
    "filename = 'softmax_loss.pdf'\n",
    "with PdfPages(filename) as pdf:\n",
    "    plt.plot(loss_hist)\n",
    "    plt.xlabel('number of epoches', fontsize = 13)\n",
    "    plt.ylabel('loss', fontsize = 13)\n",
    "    pdf.savefig(bbox_inches = 'tight')\n",
    "    plt.tick_params(axis='both', which='major', labelsize=13)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-16T04:52:10.210491100Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "xm = np.arange(-2, 18, 0.025)\n",
    "xlen = len(xm)\n",
    "ym = np.arange(-3, 11, 0.025)\n",
    "ylen = len(ym)\n",
    "xx, yy = np.meshgrid(xm, ym)\n",
    "\n",
    "\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "# xx.ravel(), yy.ravel()\n",
    "\n",
    "print(np.ones((1, xx.size)).shape)\n",
    "xx1 = xx.ravel().reshape(-1, 1)\n",
    "yy1 = yy.ravel().reshape(-1, 1)\n",
    "\n",
    "# print(xx.shape, yy.shape)\n",
    "XX = np.concatenate(( xx1, yy1, np.ones(( xx.size, 1))), axis = 1)\n",
    "\n",
    "\n",
    "print(XX.shape)\n",
    "\n",
    "Z = pred(W, XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-16T04:52:10.218546600Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "def display(X, label):\n",
    "#     K = np.amax(label) + 1\n",
    "    X0 = X[np.where(label == 0)[0]]\n",
    "    X1 = X[np.where(label == 1)[0]]\n",
    "    X2 = X[np.where(label == 2)[0]]\n",
    "    X3 = X[np.where(label == 3)[0]]\n",
    "    \n",
    "    plt.plot(X0[:, 0], X0[:, 1], 'b^', markersize = 4, alpha = .8)\n",
    "    plt.plot(X1[:, 0], X1[:, 1], 'co', markersize = 4, alpha = .8)\n",
    "    plt.plot(X2[:, 0], X2[:, 1], 'gs', markersize = 4, alpha = .8)\n",
    "    plt.plot(X3[:, 0], X3[:, 1], 'y.', markersize = 4, alpha = .8)\n",
    "    plt.plot(X4[:, 0], X4[:, 1], 'r*', markersize = 4, alpha = .8)\n",
    "\n",
    "\n",
    "#     plt.axis('off')\n",
    "    plt.plot()\n",
    "    \n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "filename = 'softmax_5class.pdf'\n",
    "with PdfPages(filename) as pdf:\n",
    "    CS = plt.contourf(xx, yy, Z, 200, cmap='jet', alpha = .1)\n",
    "\n",
    "\n",
    "    plt.xlim(-2, 18)\n",
    "    plt.ylim(-3, 11)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    # plt.axis('equal')\n",
    "    display(X, y)\n",
    "    pdf.savefig(bbox_inches = 'tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Softmax cho MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-16T04:52:10.228617700Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "mnist = fetch_mldata('MNIST original', data_home='../../data/')\n",
    "\n",
    "X = mnist.data \n",
    "y = mnist.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000)\n",
    "\n",
    "model = LogisticRegression(C = 1e5,\n",
    "        solver = 'lbfgs', multi_class = 'multinomial') # C is inverse of lam \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy %.2f %%\" % (100*accuracy_score(y_test, y_pred.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-16T04:52:10.230697400Z"
    }
   },
   "outputs": [],
   "source": [
    "# one-vs-rest logistic regression \n",
    "import numpy as np \n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "mnist = fetch_mldata('MNIST original', data_home='../../data/')\n",
    "\n",
    "X = mnist.data \n",
    "y = mnist.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000)\n",
    "\n",
    "model = LogisticRegression(C = 1e5,\n",
    "        solver = 'lbfgs', multi_class = 'ovr') # C is inverse of lam \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy %.2f %%\" % (100*accuracy_score(y_test, y_pred.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-06-16T04:52:10.240791200Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
